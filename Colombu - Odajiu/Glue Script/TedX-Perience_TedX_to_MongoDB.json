{
	"jobConfig": {
		"name": "TedX-Perience_TedX_to_MongoDB",
		"description": "",
		"role": "arn:aws:iam::702561620821:role/LabRole",
		"command": "glueetl",
		"version": "3.0",
		"runtime": null,
		"workerType": "G.1X",
		"numberOfWorkers": 10,
		"maxCapacity": 10,
		"maxRetries": 0,
		"timeout": 2880,
		"maxConcurrentRuns": 1,
		"security": "none",
		"scriptName": "TedX-Perience_TedX_to_MongoDB.py",
		"scriptLocation": "s3://tedx-perience-script/script/",
		"language": "python-3",
		"jobParameters": [],
		"tags": [],
		"jobMode": "DEVELOPER_MODE",
		"createdOn": "2023-05-18T13:09:45.259Z",
		"developerMode": true,
		"connectionsList": [],
		"temporaryDirectory": "s3://aws-glue-assets-702561620821-us-east-1/temporary/",
		"logging": true,
		"glueHiveMetastore": true,
		"etlAutoTuning": true,
		"metrics": true,
		"spark": true,
		"bookmark": "job-bookmark-disable",
		"sparkPath": "s3://tedx-perience-script/logs/",
		"flexExecution": false,
		"minFlexWorkers": null
	},
	"hasBeenSaved": false,
	"script": "import sys\nimport json\nimport pyspark\nfrom pyspark.sql.functions import col, collect_list, collect_set, regexp_replace, sort_array, struct, explode, monotonically_increasing_id, row_number\nfrom pyspark.sql.window import Window\n\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\n\n############################## FUNZIONI #####################################\n\ndef calc_avg_views(num_views, views):\n    for a in range(len(num_views)):\n        if(num_views[a]==0):\n            num_views[a]=views[a]\n        if(views[a]==0):\n            views[a]=num_views[a]\n    return [(a+b)/2 for a, b in zip(num_views, views)]\n\n#############################################################################\n\n##### FROM FILES\ntedx_dataset_path = \"s3://tedx-perience-data/tedx_dataset.csv\"\n\n##### READ PARAMETERS\nargs = getResolvedOptions(sys.argv, ['JOB_NAME'])\n\n##### START JOB CONTEXT AND JOB\nsc = SparkContext()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)\njob.init(args['JOB_NAME'], args)\njob.commit()\n\n##### READ INPUT FILES TO CREATE AN INPUT DATASET\ntedx_dataset = spark.read \\\n    .option(\"header\",\"true\") \\\n    .option(\"quote\",\"\\\"\") \\\n    .option(\"escape\",\"\\\"\") \\\n    .csv(tedx_dataset_path)\n    \ntedx_dataset.printSchema()\n\n##### FILTER ITEMS WITH NULL POSTING KEY\ncount_items = tedx_dataset.count()\ncount_items_null = tedx_dataset.filter(\"idx is not null\").count()\n\nprint(f\"Number of items from RAW DATA {count_items}\")\nprint(f\"Number of items from RAW DATA with NOT NULL KEY {count_items_null}\")\n\n##### READ TAGS DATASET\ntags_dataset_path = \"s3://tedx-perience-data/tags_dataset.csv\"\ntags_dataset = spark.read.option(\"header\", \"true\").csv(tags_dataset_path)\n\n##### CREATE THE AGGREGATE MODEL, ADD TAGS TO TEDX DATASET\ntags_dataset_agg = tags_dataset.groupBy(col(\"idx\").alias(\"idx_ref\")).agg(collect_list(\"tag\").alias(\"tags\"))\ntags_dataset_agg.printSchema()\ntedx_dataset_agg = tedx_dataset.join(tags_dataset_agg, tedx_dataset.idx == tags_dataset_agg.idx_ref, \"left\") \\\n    .drop(\"idx_ref\") \\\n    .select(col(\"idx\").alias(\"_id\"), col(\"*\")) \\\n    .drop(\"idx\") \\\n\ntedx_dataset_agg.printSchema()\n\n##### READ WATCH NEXT DATASET\nwatch_next_dataset_path = \"s3://tedx-perience-data/watch_next_dataset.csv\"\nwatch_next_dataset = spark.read.option(\"header\", \"true\").csv(watch_next_dataset_path)\n\n##### CREATE THE AGGREGATE MODEL WATCH NEXT\nwatch_next_dataset_agg = watch_next_dataset.groupBy(col(\"idx\").alias(\"idx_ref\")).agg(collect_set(\"watch_next_idx\").alias(\"watch_next\"))\nwatch_next_dataset_agg.printSchema()\ntedx_dataset_agg = tedx_dataset_agg.join(watch_next_dataset_agg, tedx_dataset_agg._id == watch_next_dataset_agg.idx_ref, \"left\") \\\n    .drop(\"idx_ref\") \\\n    \n##### READ FILE DATA\ntedx_newdataset_path = \"s3://tedx-perience-data/data.csv\"\ntedx_newdataset = spark.read \\\n    .option(\"header\",\"true\") \\\n    .option(\"quote\",\"\\\"\") \\\n    .option(\"escape\",\"\\\"\") \\\n    .option(\"delimiter\", \",\") \\\n    .csv(tedx_newdataset_path)\ntedx_newdataset = tedx_newdataset.join(tedx_dataset_agg, tedx_newdataset.title == tedx_dataset_agg.title, \"inner\")\n\ntedx_newdataset = (\n    tedx_newdataset\n    .withColumn('num_views', regexp_replace('num_views', ',', ''))\n    .withColumn('num_views', col('num_views').cast(\"int\"))\n)\n\ntedx_newdataset = (\n    tedx_newdataset\n    .withColumn('views', regexp_replace('views', ',', ''))\n    .withColumn('views', col('views').cast(\"int\"))\n)\n\n##### SAVE DATA AS LISTS\nnum_views = tedx_newdataset.select(\"num_views\").rdd.flatMap(lambda x: x).map(lambda x: int(x) if x else 0).collect()\nviews =  tedx_newdataset.select(\"views\").rdd.flatMap(lambda x: x).map(lambda x: int(x) if x else 0).collect()\n\n##### FIND AVERAGE VIEWS\navg_views_list = calc_avg_views(num_views, views)\ndf_avg_views = spark \\\n    .createDataFrame([(avg_views_list,)], ['lista'])\\\n    .select(explode(col('lista')).alias('avg_views'))\n    \n##### ADD INDEX TO DATA FRAME TO JOIN\ndf_avg_views = df_avg_views.withColumn(\"index\", row_number().over(Window.orderBy(monotonically_increasing_id())))\ntedx_newdataset = tedx_newdataset.withColumn(\"index\", row_number().over(Window.orderBy(monotonically_increasing_id())))\n\n##### JOIN DATA FRAMES\ntedx_newdataset_complete = tedx_newdataset \\\n    .join(df_avg_views, tedx_newdataset.index == df_avg_views.index, \"inner\") \\\n    .drop(\"index\") \\\n    .withColumnRenamed(\"idx\", \"idx_ref\")\n\n##### STORE DATA ON MONGODB\nmongo_uri = \"mongodb+srv://Ombuman:admin123@tedxcluster.lzu3kfz.mongodb.net\"\nprint(mongo_uri)\n\nwrite_mongo_options = {\n    \"uri\": mongo_uri,\n    \"database\": \"unibg_tedx_2023\",\n    \"collection\": \"tedx_data\",\n    \"ssl\": \"true\",\n    \"ssl.domain_match\": \"false\"}\nfrom awsglue.dynamicframe import DynamicFrame\ntedx_dataset_dynamic_frame = DynamicFrame.fromDF(tedx_newdataset_complete, glueContext, \"nested\")\n\nglueContext.write_dynamic_frame.from_options(tedx_dataset_dynamic_frame, connection_type=\"mongodb\", connection_options=write_mongo_options)"
}